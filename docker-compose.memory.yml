services:
  # =============================================================================
  # LLM Service (Ollama for local testing)
  # =============================================================================

  ollama:
    image: ollama/ollama:latest
    container_name: fidus-memory-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 0"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # =============================================================================
  # Fidus Memory API
  # =============================================================================

  memory-api:
    build:
      context: .
      dockerfile: packages/api/Dockerfile
    container_name: fidus-memory-api
    restart: unless-stopped
    environment:
      # LLM Configuration
      FIDUS_LLM_MODEL: ${FIDUS_LLM_MODEL:-ollama/llama3.2:3b}
      OLLAMA_API_BASE: http://ollama:11434

      # OpenAI API (optional, if you want to use OpenAI instead)
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}

      # Anthropic API (optional, if you want to use Claude instead)
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}

      # Application
      ENVIRONMENT: ${ENVIRONMENT:-production}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}

      # CORS
      CORS_ORIGINS: http://localhost:3001,http://memory-web:3000
    ports:
      - "8000:8000"
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # =============================================================================
  # Fidus Memory Web Frontend
  # =============================================================================

  memory-web:
    build:
      context: .
      dockerfile: packages/web/Dockerfile
    container_name: fidus-memory-web
    restart: unless-stopped
    environment:
      # Backend API URL
      BACKEND_URL: http://memory-api:8000

      # Next.js
      NODE_ENV: production
      NEXT_PUBLIC_API_URL: http://localhost:8000
    ports:
      - "3001:3000"
    depends_on:
      memory-api:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

# =============================================================================
# Volumes
# =============================================================================

volumes:
  ollama_data:
    driver: local

# =============================================================================
# Networks
# =============================================================================

networks:
  default:
    name: fidus-memory-network
    driver: bridge
